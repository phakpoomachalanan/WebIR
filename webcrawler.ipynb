{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "BASE_PATH = os.getenv(\"BASE_PATH\")\n",
    "MAX_PAGE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Meow\\'s Bot',\n",
    "    'From': 'test@domain.com'\n",
    "}\n",
    "# seed_url = 'http://www.ku.ac.th/th/'\n",
    "seed_url = \"https://crawler-test.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    global headers, counter\n",
    "    text = ''\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=2)\n",
    "        # If the response was successful, no Exception will be raised\n",
    "        response.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')  # Python 3.6\n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')  # Python 3.6\n",
    "    else:\n",
    "        print(f'Success!: {url}')\n",
    "        text = response.text\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def get_base_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.scheme, parsed_url.netloc\n",
    "\n",
    "def link_parser(raw_html):\n",
    "    urls = [];\n",
    "    pattern_start = '<a href=\"';  pattern_end = '\"'\n",
    "    index = 0;  length = len(raw_html)\n",
    "    while index < length:\n",
    "        start = raw_html.find(pattern_start, index)\n",
    "        if start > 0:\n",
    "            start = start + len(pattern_start)\n",
    "            end = raw_html.find(pattern_end, start)\n",
    "            link = raw_html[start:end]\n",
    "            if len(link) > 0:\n",
    "                if link not in urls:\n",
    "                    urls.append(link)\n",
    "            index = end\n",
    "        else:\n",
    "            break\n",
    "    return urls\n",
    "\n",
    "def enqueue(links):\n",
    "    global frontier_q, visited_q\n",
    "    for link in links:\n",
    "        if link not in frontier_q and link not in visited_q:\n",
    "            frontier_q.append(link)\n",
    "\n",
    "def dequeue():\n",
    "    global frontier_q\n",
    "    current_url = frontier_q[0]\n",
    "    frontier_q = frontier_q[1:]\n",
    "    return current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(page, url):\n",
    "    global BASE_PATH\n",
    "    path = os.path.join(BASE_PATH, url)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(page)\n",
    "\n",
    "def get_dothtml_from_url(url):\n",
    "    path = urlparse(url).path\n",
    "    dothtml = path.split('/')[-1]\n",
    "    return dothtml if (dothtml.endswith(\".html\") or dothtml.endswith(\".htm\")) else \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier_q = [seed_url]\n",
    "visited_q = []\n",
    "counter = 0\n",
    "\n",
    "while ((len(frontier_q) != 0) and counter < MAX_PAGE):\n",
    "    current_url = dequeue()\n",
    "    scheme, base_url = get_base_url(current_url)\n",
    "    if (base_url not in visited_q):\n",
    "        visited_q.append(base_url)\n",
    "        url = urljoin(scheme + \"://\" + base_url, \"robots.txt\")\n",
    "        page = get_page(url)\n",
    "        if (\"user-agent:\" in page):\n",
    "            create_file(page, os.path.join(base_url, \"robots.txt\"))\n",
    "            # TODO: record web that has robots.txt in it \n",
    "        \n",
    "        # TODO: extract sitemap\n",
    "        # TODO: record web that has sitemap\n",
    "\n",
    "    visited_q.append(current_url)\n",
    "    raw_html = get_page(current_url)\n",
    "    if (raw_html != \"\"):\n",
    "        filename = get_dothtml_from_url(current_url)\n",
    "        if (filename == \"dummy\"):\n",
    "            filename = urljoin(\"\".join(current_url.split(\"://\")[1:]), \"dummy\")\n",
    "        else:\n",
    "            filename = \"\".join(current_url.split(\"://\")[1:])\n",
    "        create_file(raw_html, os.path.join(filename))\n",
    "\n",
    "    extracted_links = link_parser(raw_html)\n",
    "    enqueue([urljoin(current_url, link) for link in extracted_links])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
