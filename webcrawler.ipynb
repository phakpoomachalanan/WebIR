{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "from urllib.parse import urlparse, urljoin, unquote, parse_qs, urlencode\n",
    "import html\n",
    "\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import urllib.robotparser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "BASE_PATH = os.getenv(\"BASE_PATH\")\n",
    "BASE_HTML_PATH = os.getenv(\"BASE_HTML_PATH\")\n",
    "MAX_PAGE = int(os.getenv(\"MAX_PAGE\"))\n",
    "POOL_SIZE = int(os.getenv(\"POOL_SIZE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefectures = [\n",
    "    \"Hokkaido\", \"Aomori\", \"Iwate\", \"Miyagi\", \"Akita\", \"Yamagata\", \"Fukushima\",\n",
    "    \"Ibaraki\", \"Tochigi\", \"Gunma\", \"Saitama\", \"Chiba\", \"Tokyo\", \"Kanagawa\", \n",
    "    \"Niigata\", \"Toyama\", \"Ishikawa\", \"Fukui\", \"Yamanashi\", \"Nagano\", \"Gifu\", \n",
    "    \"Shizuoka\", \"Aichi\", \"Mie\", \"Shiga\", \"Kyoto\", \"Osaka\", \"Hyogo\", \"Nara\", \n",
    "    \"Wakayama\", \"Tottori\", \"Shimane\", \"Okayama\", \"Hiroshima\", \"Yamaguchi\", \n",
    "    \"Tokushima\", \"Kagawa\", \"Ehime\", \"Kochi\", \"Fukuoka\", \"Saga\", \"Nagasaki\", \n",
    "    \"Kumamoto\", \"Oita\", \"Miyazaki\", \"Kagoshima\", \"Okinawa\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'WEBIR_2 KU Project\\'s Bot',\n",
    "    'From': 'phakpoom.a@ku.th'\n",
    "}\n",
    "seed_urls = [input()]\n",
    "# seed_urls = [\"https://www.japan.travel/en/us/\", \"https://japantravel.navitime.com/en/\", \"https://www.japan-guide.com/\", \"https://en.japantravel.com/\", \"https://www.ana.co.jp/en/us/japan-travel-planner/\"]\n",
    "# seed_urls = [\"https://www.aichi-now.jp/en/\", \"https://enjoy.pref.fukui.lg.jp/en/\", \"https://visitgifu.com/\", \"https://www.ishikawatravel.jp/en/\", \"https://www.go-nagano.net/en/\", \"http://en.nagano-cvb.or.jp/\", \"https://nagoya.travel/\", \"https://enjoyniigata.com/en/\", \"https://www.nvcb.or.jp/multilingual/\", \"https://www.visitsado.com/en/\", \"https://www.visit-shizuoka.com/en/\", \"https://exploreshizuoka.jp/en/\", \"https://www.toyamashi-kankoukyoukai.jp/en/\", \"https://visit-toyama-japan.com/en\", \"https://www.yamanashi-kankou.jp/english/index.html\", \"https://dive-hiroshima.com/en/\", \"https://www.okayama-japan.jp/en\", \"https://www.kankou-shimane.com/en/\", \"https://www.tottori-tour.jp/en/\", \"https://yamaguchi-city.jp/w/en/\", \"https://www.visit-jy.com/en/\", \"https://visit.sapporo.travel/\", \"https://www.visit-hokkaido.jp/en/index.html\", \"https://www.visitchiba.jp\", \"https://www.visit-gunma.jp/en/\", \"https://visit.ibarakiguide.jp/en/\", \"https://trip.pref.kanagawa.jp\", \"https://www.stib.jp/saitamacity-visitorsguide/\", \"https://saitama-supportdesk.com/\", \"https://www.visit-tochigi.com\", \"https://www.gotokyo.org/en/index.html\", \"https://www.hyogo-tourism.jp/world/\", \"https://www.kyototourism.org/en/\", \"https://www.kyoto.travel/en/\", \"https://www.visitnara.jp/\", \"https://en.osaka-info.jp/\", \"https://en.biwako-visitors.jp/\", \"https://visitwakayama.jp/en/index.html\", \"https://visitmie-japan.travel/en/index.html\", \"https://www.travel.pref.mie.lg.jp/en/index.shtm\", \"https://www.crossroadfukuoka.jp/en\", \"https://gofukuoka.jp\", \"https://www.kagoshima-yokanavi.jp/en\", \"https://www.kagoshima-kankou.com/for\", \"https://kumamoto.guide/en/\", \"https://kumamoto-guide.jp/en/\", \"https://www.miyazaki-city.tourism.or.jp/en\", \"https://www.kanko-miyazaki.jp/en\", \"https://www.discover-nagasaki.com/en\", \"https://en.at-nagasaki.jp\", \"https://www.discover-oita.com\", \"https://visitokinawajapan.com\", \"https://www.asobo-saga.jp/en/\", \"https://visitehimejapan.com/en/\", \"https://cycling-ehime.com/en/?_ga=2.264379849.717529468.1708478125-1172468562.1680245483\", \"https://www.my-kagawa.jp/en\", \"https://visitkochijapan.com/en/\", \"https://www.pref.tokushima.lg.jp/en/japanese/tourism\", \"https://shikoku-tourism.com/en/\", \"https://visitakita.com/en/\", \"https://aomori-tourism.com/en/\", \"https://fukushima.travel\", \"https://iwatetabi.jp/en/\", \"https://visitmiyagi.com\", \"https://yamagatakanko.com/en/\", \"https://www.tohokukanko.jp/en/\"]\n",
    "seed_domain = [(urlparse(url)).netloc for url in seed_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_english_page(response):\n",
    "    try:\n",
    "        content_language = response.headers.get(\"Content-Language\")\n",
    "        if content_language and \"en\" not in content_language.lower():\n",
    "            return False\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        html_lang = soup.html.get(\"lang\")\n",
    "        if html_lang and not (html_lang.startswith(\"en\") or html_lang.startswith(\"en-us\")):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    global headers, counter\n",
    "    response = None\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=2)\n",
    "        response.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')\n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')\n",
    "    else:\n",
    "        if (is_english_page(response) or url.endswith(\"robots.txt\")):\n",
    "            print(f'Success!: {(counter+1):10}, {url}')\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    return response\n",
    "\n",
    "def get_base_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.scheme, parsed_url.netloc\n",
    "\n",
    "def link_parser(raw_html):\n",
    "    urls = []\n",
    "    pattern_start = '<a href=\"';  pattern_end = '\"'\n",
    "    index = 0;  length = len(raw_html)\n",
    "    while index < length:\n",
    "        start = raw_html.find(pattern_start, index)\n",
    "        if start > 0:\n",
    "            start = start + len(pattern_start)\n",
    "            end = raw_html.find(pattern_end, start)\n",
    "            link = raw_html[start:end]\n",
    "            if len(link) > 0:\n",
    "                if link not in urls:\n",
    "                    urls.append(link)\n",
    "            index = end\n",
    "        else:\n",
    "            break\n",
    "    return urls\n",
    "\n",
    "def normalize_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    \n",
    "    unique_params = {key: value[0] for key, value in query_params.items()}\n",
    "\n",
    "    normalized_query = urlencode(unique_params, doseq=True)\n",
    "    normalized_url = parsed_url._replace(query=normalized_query).geturl()\n",
    "\n",
    "    parsed_url = urlparse(normalized_url)\n",
    "    path_parts = parsed_url.path.split('/')\n",
    "    normalized_path = '/'.join(sorted(set(path_parts), key=path_parts.index))\n",
    "    normalized_url = parsed_url._replace(path=normalized_path).geturl()\n",
    "    return normalized_url\n",
    "\n",
    "def enqueue(links, base_urls):\n",
    "    global frontier_q, visited_q\n",
    "    for link in links:\n",
    "        if link not in frontier_q and link not in visited_q:\n",
    "            if any(base_url in link for base_url in base_urls):\n",
    "                frontier_q = [normalize_url(link)] + frontier_q\n",
    "\n",
    "def dequeue():\n",
    "    global frontier_q\n",
    "    current_url = frontier_q[0]\n",
    "    frontier_q = frontier_q[1:]\n",
    "    return current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(data, url):\n",
    "    global BASE_PATH\n",
    "    try: \n",
    "        path = os.path.join(BASE_PATH, url)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, \"w\") as fp:\n",
    "            fp.write(data)\n",
    "    except OSError as os_err:\n",
    "        print(os_err)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_dothtml_from_url(url):\n",
    "    path = urlparse(url).path\n",
    "    last_path = path.split('/')[-1]\n",
    "    if (last_path.endswith(\".html\") or last_path.endswith(\".htm\") or last_path.endswith(\".shtm\")):\n",
    "        return last_path\n",
    "    elif (\".\" in last_path):\n",
    "        return None\n",
    "    else: \n",
    "        return \"dummy\"\n",
    "    \n",
    "def decode_html_url(url):\n",
    "    url = html.unescape(url)\n",
    "    url = unquote(url)\n",
    "    return url\n",
    "\n",
    "def create_abs_url(current_url, link):\n",
    "    return urljoin(current_url, decode_html_url(link))\n",
    "\n",
    "def remove_header(content):\n",
    "    content = re.sub(r'(?s)<header.*?>.*?</header>', '', content)\n",
    "    return content\n",
    "\n",
    "def remove_footer(content):\n",
    "    content = re.sub(r'(?s)<footer.*?>.*?</footer>', '', content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier_q = seed_urls\n",
    "visited_q = set()\n",
    "counter = 0\n",
    "has_robotstxt = list()\n",
    "has_sitemap = set()\n",
    "disallow = set()\n",
    "\n",
    "user_agent = headers[\"User-Agent\"]\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "\n",
    "scheme, base_url = get_base_url(frontier_q[0])\n",
    "\n",
    "flag = True\n",
    "url = urljoin(scheme + \"://\" + base_url, \"robots.txt\")\n",
    "if (url not in visited_q):\n",
    "    visited_q.add(url)\n",
    "    page = get_page(url)\n",
    "    if (page is not None):\n",
    "        page = page.text\n",
    "        if (\"User-agent:\" in page):\n",
    "            create_file(page, os.path.join(BASE_HTML_PATH, base_url, \"robots.txt\"))\n",
    "            has_robotstxt.append(base_url)\n",
    "            flag = False\n",
    "    rp.set_url(url)\n",
    "    rp.read()\n",
    "\n",
    "# base_urls = [\"https://en.japantravel.com/\" + prefecture.lower() for prefecture in prefectures]\n",
    "# base_urls = [f\"https://www.japan-guide.com/{path}\" for path in [\"blog\", \"bus\", \"chottozeitaku\", \"event\", \"forum\", \"list\"]]\n",
    "# base_urls = [\"https://japantravel.navitime.com/en/area/jp/destinations\", \"https://japantravel.navitime.com/en/area/jp/guide\", \"https://japantravel.navitime.com/en/area/jp/spot\"]\n",
    "\n",
    "while ((len(frontier_q) != 0) and counter < MAX_PAGE):\n",
    "    current_url = dequeue()\n",
    "    if (current_url in visited_q):\n",
    "        continue\n",
    "\n",
    "    if (rp.can_fetch(user_agent, current_url) or flag):\n",
    "        visited_q.add(current_url)\n",
    "        response = get_page(current_url)\n",
    "        if (response is None):\n",
    "            continue\n",
    "        raw_html = response.text\n",
    "        filename = get_dothtml_from_url(current_url)\n",
    "        if (filename is None):\n",
    "            pass\n",
    "        else:\n",
    "            if (filename == \"dummy\"):\n",
    "                filename = os.path.join(\"\".join(current_url.split(\"://\")[1:]), \"dummy\")\n",
    "            else:\n",
    "                filename = \"\".join(current_url.split(\"://\")[1:])\n",
    "            success = create_file(remove_header(remove_footer(raw_html)), os.path.join(BASE_HTML_PATH, filename))\n",
    "            if (success):\n",
    "                counter += 1\n",
    "    \n",
    "        extracted_links = link_parser(raw_html)\n",
    "        for link in extracted_links:\n",
    "            link = create_abs_url(current_url, link)\n",
    "            ext = get_dothtml_from_url(link)\n",
    "            if (ext is None):\n",
    "                continue\n",
    "            if (link.split(\"/\")[-1].find(\"#\") != -1):\n",
    "                continue\n",
    "            enqueue([link], base_urls)\n",
    "\n",
    "if (create_file(\"\\n\".join(has_robotstxt), os.path.join(BASE_PATH, \"list_robots.txt\"))):\n",
    "    print(\"Done!\", seed_urls[0], counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
