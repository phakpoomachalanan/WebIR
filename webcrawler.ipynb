{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "BASE_PATH = os.getenv(\"BASE_PATH\")\n",
    "BASE_HTML_PATH = os.getenv(\"BASE_HTML_PATH\")\n",
    "MAX_PAGE = os.getenv(\"MAX_PAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Meow\\'s Bot',\n",
    "    'From': 'test@domain.com'\n",
    "}\n",
    "seed_url = 'http://www.ku.ac.th/th/'\n",
    "# seed_url = \"https://crawler-test.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    global headers, counter\n",
    "    text = ''\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=2)\n",
    "        # If the response was successful, no Exception will be raised\n",
    "        response.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')  # Python 3.6\n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')  # Python 3.6\n",
    "    else:\n",
    "        print(f'Success!: {(counter+1):5}, {url}')\n",
    "        text = response.text\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_base_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.scheme, parsed_url.netloc\n",
    "\n",
    "def link_parser(raw_html):\n",
    "    urls = [];\n",
    "    pattern_start = '<a href=\"';  pattern_end = '\"'\n",
    "    index = 0;  length = len(raw_html)\n",
    "    while index < length:\n",
    "        start = raw_html.find(pattern_start, index)\n",
    "        if start > 0:\n",
    "            start = start + len(pattern_start)\n",
    "            end = raw_html.find(pattern_end, start)\n",
    "            link = raw_html[start:end]\n",
    "            if len(link) > 0:\n",
    "                if link not in urls:\n",
    "                    urls.append(link)\n",
    "            index = end\n",
    "        else:\n",
    "            break\n",
    "    return urls\n",
    "\n",
    "def enqueue(links):\n",
    "    global frontier_q, visited_q\n",
    "    for link in links:\n",
    "        if link not in frontier_q and link not in visited_q:\n",
    "            frontier_q.append(link)\n",
    "\n",
    "def dequeue():\n",
    "    global frontier_q\n",
    "    current_url = frontier_q[0]\n",
    "    frontier_q = frontier_q[1:]\n",
    "    return current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(data, url):\n",
    "    global BASE_PATH\n",
    "    path = os.path.join(BASE_PATH, url)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(data)\n",
    "\n",
    "def get_dothtml_from_url(url):\n",
    "    path = urlparse(url).path\n",
    "    dothtml = path.split('/')[-1]\n",
    "    return dothtml if (dothtml.endswith(\".html\") or dothtml.endswith(\".htm\")) else \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!:     1, https://crawler-test.com/robots.txt\n",
      "Success!:     1, https://crawler-test.com/\n",
      "Success!:     2, https://crawler-test.com/mobile/separate_desktop\n",
      "Success!:     3, https://crawler-test.com/mobile/desktop_with_AMP_as_mobile\n",
      "Success!:     4, https://crawler-test.com/mobile/separate_desktop_with_different_h1\n",
      "Success!:     5, https://crawler-test.com/mobile/separate_desktop_with_different_title\n",
      "Success!:     6, https://crawler-test.com/mobile/separate_desktop_with_different_wordcount\n",
      "Success!:     7, https://crawler-test.com/mobile/separate_desktop_with_different_links_in\n",
      "Success!:     8, https://crawler-test.com/mobile/separate_desktop_with_different_links_out\n",
      "Success!:     9, https://crawler-test.com/mobile/separate_desktop_with_mobile_not_subdomain\n",
      "Success!:    10, https://crawler-test.com/mobile/desktop_with_self_canonical_mobile_and_amp\n",
      "Success!:    11, https://crawler-test.com/mobile/separate_mobile_with_mobile_not_subdomain\n",
      "Success!:    12, https://crawler-test.com/mobile/dynamic\n",
      "Success!:    13, https://crawler-test.com/mobile/responsive\n",
      "Success!:    14, https://crawler-test.com/mobile/no_mobile_configuration\n",
      "Success!:    15, https://crawler-test.com/mobile/other_desktop_that_links_to_the_same_mobile_pages\n",
      "Success!:    16, https://crawler-test.com/mobile/amp_with_separate_mobile\n",
      "Success!:    17, https://crawler-test.com/mobile/responsive_with_amp\n",
      "Success!:    18, https://crawler-test.com/mobile/amp_with_responsive\n",
      "Success!:    19, https://crawler-test.com/mobile/no_mobile_with_amp\n",
      "Success!:    20, https://crawler-test.com/mobile/amp_with_no_mobile\n",
      "Success!:    21, https://crawler-test.com/mobile/amp_no_references\n",
      "Success!:    22, https://crawler-test.com/mobile/amp_as_desktop_amp_and_mobile\n",
      "Success!:    23, https://crawler-test.com/mobile/separate_amp_with_self_canonical\n",
      "Success!:    24, https://crawler-test.com/mobile/separate_desktop_irregular_media\n",
      "Success!:    25, https://crawler-test.com/mobile/separate_desktop_response_header_alt\n",
      "Success!:    26, https://crawler-test.com/description_tags/description_with_whitespace\n",
      "Success!:    27, https://crawler-test.com/description_tags/missing_description\n",
      "Success!:    28, https://crawler-test.com/description_tags/no_description_nosnippet\n",
      "Success!:    29, https://crawler-test.com/description_tags/duplicate_description\n",
      "Success!:    30, https://crawler-test.com/description_tags/duplicate_description/foo\n",
      "Success!:    31, https://crawler-test.com/description_tags/duplicate_description_and_noindex\n",
      "Success!:    32, https://crawler-test.com/description_tags/duplicate_description_and_noindex/foo\n",
      "Success!:    33, https://crawler-test.com/description_tags/description_over_max\n",
      "Success!:    34, https://crawler-test.com/description_tags/short_meta_description\n",
      "Success!:    35, https://crawler-test.com/description_tags/description_http_equiv\n",
      "Success!:    36, https://crawler-test.com/encoding/page_titles_character_encoded\n",
      "Success!:    37, https://crawler-test.com/encoding/url_with_foreign_characters/בלהבלה\n",
      "Success!:    38, https://crawler-test.com/encoding/url_with_foreign_characters/すべての単語が高校程度の辞書に載っている\n",
      "Success!:    39, https://crawler-test.com/encoding/url_with_foreign_characters/pchnąć-w-tę-łódź-jeża-lub-ośm-skrzyń-fig\n",
      "Success!:    40, https://crawler-test.com/encoding/url_with_foreign_characters/Шеф-взъярён-тчк-щипцы-с-эхом-гудбай-Жюль\n",
      "Success!:    41, https://crawler-test.com/encoding/url_with_foreign_characters/Zwölf-große-Boxkämpfer-jagen-Viktor-quer-über-den-Sylter-Deich\n",
      "Success!:    42, https://crawler-test.com/encoding/url_with_foreign_characters/Fabio-me-exige-sin-tapujos-que-añada-cerveza-al-whisky\n",
      "Success!:    43, https://crawler-test.com/encoding/url_with_foreign_characters/﴿محمد-رسول-الله-والذين-معه-أشداء\n",
      "Success!:    44, https://crawler-test.com/encoding/url_with_foreign_characters/γράμματα-του-ισπανικού-αλφαβήτου-καθώς\n",
      "Success!:    45, https://crawler-test.com/encoding/url_with_foreign_characters/asød-æada-ådjghf-gägfd-asödsads\n",
      "Success!:    46, https://crawler-test.com/encoding/double_encoded_url/Zw%25C3%25B6lf-gro%25C3%259Fe-Boxk%25C3%25A4mpfer-jagen-Viktor-quer-%25C3%25BCber-den-Sylter-Deich\n",
      "Success!:    47, https://crawler-test.com/encoding/inconsistent_character_encoding\n",
      "Success!:    48, https://crawler-test.com/encoding/url/encoded_hashbang%23abc\n",
      "Success!:    49, https://crawler-test.com/titles/title_with_whitespace\n",
      "Success!:    50, https://crawler-test.com/titles/empty_title\n",
      "Success!:    51, https://crawler-test.com/titles/missing_title\n",
      "Success!:    52, https://crawler-test.com/titles/duplicate_title\n",
      "Success!:    53, https://crawler-test.com/titles/duplicate_title/foo\n",
      "Success!:    54, https://crawler-test.com/titles/duplicate_title/bar\n",
      "Success!:    55, https://crawler-test.com/titles/duplicate_title/baz\n",
      "Success!:    56, https://crawler-test.com/titles/duplicate_title_and_noindex/bat\n",
      "Success!:    57, https://crawler-test.com/titles/duplicate_title_and_noindex/bak\n",
      "Success!:    58, https://crawler-test.com/titles/title_over_max\n",
      "Success!:    59, https://crawler-test.com/titles/title_warning\n",
      "Success!:    60, https://crawler-test.com/titles/page_title_length_n\n",
      "Success!:    61, https://crawler-test.com/titles/page_title_width_n\n",
      "Success!:    62, https://crawler-test.com/titles/page_title_leading_trailing_spaces\n",
      "Success!:    63, https://crawler-test.com/titles/double_triple_quadruple_spaces\n",
      "Success!:    64, https://crawler-test.com/titles/svg_title\n",
      "Success!:    65, https://crawler-test.com/titles/forced_double_triple_quadruple_spaces\n",
      "Success!:    66, https://crawler-test.com/robots_protocol/robots_excluded\n",
      "Success!:    67, https://crawler-test.com/robots_protocol/deepcrawl_excluded\n",
      "Success!:    68, https://crawler-test.com/robots_protocol/robots_excluded_duplicate_description\n",
      "Success!:    69, https://crawler-test.com/robots_protocol/robots_excluded_meta_noindex\n",
      "HTTP error occurred: 404 Client Error: Not Found for url: https://crawler-test.com/robots_protocol/deepcrawl_ua_disallow/foo\n",
      "Success!:    70, https://crawler-test.com/robots_protocol/user_excluded\n",
      "Success!:    71, https://crawler-test.com/robots_protocol/meta_nofollow\n",
      "Success!:    72, https://crawler-test.com/robots_protocol/meta_noarchive\n",
      "Success!:    73, https://crawler-test.com/robots_protocol/meta_noindex\n",
      "Success!:    74, https://crawler-test.com/robots_protocol/meta_noindex_uppercase\n",
      "Success!:    75, https://crawler-test.com/robots_protocol/x_robots_tag_noindex\n",
      "Success!:    76, https://crawler-test.com/robots_protocol/page_allowed_with_robots\n",
      "Success!:    77, https://crawler-test.com/robots_protocol/robots_noindexed\n",
      "Success!:    78, https://crawler-test.com/robots_protocol/robots_noindex_conflict\n",
      "Success!:    79, https://crawler-test.com/robots_protocol/robots_excluded_blank_line\n",
      "Success!:    80, https://crawler-test.com/robots_protocol/robots_noindexed_and_robots_disallowed\n",
      "Success!:    81, https://crawler-test.com/robots_protocol/allowed_same_length\n",
      "Success!:    82, https://crawler-test.com/robots_protocol/allowed_shorter\n",
      "Success!:    83, https://crawler-test.com/robots_protocol/allowed_longer\n",
      "Success!:    84, https://crawler-test.com/robots_protocol/meta_robots_and_x_robots_conflict\n",
      "Success!:    85, https://crawler-test.com/robots_protocol/robots_meta_none\n",
      "Success!:    86, https://crawler-test.com/robots_protocol/robots_meta_noodp_noydir_none_noindex\n",
      "Success!:    87, https://crawler-test.com/robots_protocol/robots_meta_multiple_tags_noindex_nofollow\n",
      "Success!:    88, https://crawler-test.com/robots_protocol/x_robots_multiple_directives\n",
      "Success!:    89, https://crawler-test.com/robots_protocol/multiple_robots_directive_meta_tag\n",
      "Success!:    90, https://crawler-test.com/robots_protocol/multiple_googlebot_directive_meta_tag\n",
      "HTTP error occurred: 404 Client Error: Not Found for url: https://crawler-test.com/robots_protocol/non_200_with_noindex\n",
      "Success!:    91, https://crawler-test.com/robots_protocol/canonicalised_with_noindex\n",
      "HTTP error occurred: 404 Client Error: Not Found for url: https://crawler-test.com/robots_protocol/canonicalised_with_non_200\n",
      "Success!:    92, https://crawler-test.com/redirects/redirect_1\n",
      "Success!:    93, https://crawler-test.com/redirects/redirect_2\n",
      "Success!:    94, https://crawler-test.com/redirects/redirect_3_302\n",
      "Success!:    95, https://crawler-test.com/redirects/redirect_4_307\n",
      "Success!:    96, https://crawler-test.com/redirects/disallowed_redirect\n",
      "Success!:    97, https://crawler-test.com/redirects/redirect_chain_allowed\n",
      "Success!:    98, https://crawler-test.com/redirects/disallowed_redirect_target_redirect\n",
      "Other error occurred: Exceeded 30 redirects.\n",
      "Other error occurred: Exceeded 30 redirects.\n",
      "Success!:    99, https://crawler-test.com/redirects/external_redirect\n",
      "HTTP error occurred: 404 Client Error: Not Found for url: https://crawler-test.com/images/foo.jpg\n",
      "Success!:   100, https://crawler-test.com/redirects/meta_redirect_1\n"
     ]
    }
   ],
   "source": [
    "frontier_q = [seed_url]\n",
    "visited_q = list()\n",
    "counter = 0\n",
    "has_robotstxt = list()\n",
    "disallow = list()\n",
    "\n",
    "while ((len(frontier_q) != 0) and counter < MAX_PAGE):\n",
    "    current_url = dequeue()\n",
    "    if (current_url in visited_q):\n",
    "        continue\n",
    "\n",
    "    scheme, base_url = get_base_url(current_url)\n",
    "    if (base_url not in visited_q):\n",
    "        visited_q.append(base_url)\n",
    "        url = urljoin(scheme + \"://\" + base_url, \"robots.txt\")\n",
    "        page = get_page(url)\n",
    "        if (\"User-agent:\" in page):\n",
    "            create_file(page, os.path.join(BASE_HTML_PATH, base_url, \"robots.txt\"))\n",
    "            has_robotstxt.append(base_url)\n",
    "\n",
    "        # TODO: extract sitemap\n",
    "        # TODO: record web that has sitemap\n",
    "\n",
    "    visited_q.append(current_url)\n",
    "    raw_html = get_page(current_url)\n",
    "    if (raw_html != \"\"):\n",
    "        counter += 1\n",
    "        filename = get_dothtml_from_url(current_url)\n",
    "        if (filename == \"dummy\"):\n",
    "            filename = os.path.join(\"\".join(current_url.split(\"://\")[1:]), \"dummy\")\n",
    "        else:\n",
    "            filename = \"\".join(current_url.split(\"://\")[1:])\n",
    "        create_file(raw_html, os.path.join(BASE_HTML_PATH, filename))\n",
    "\n",
    "    extracted_links = link_parser(raw_html)\n",
    "    enqueue([urljoin(current_url, link) for link in extracted_links])\n",
    "\n",
    "\n",
    "\n",
    "create_file(\"\\n\".join(has_robotstxt), os.path.join(BASE_PATH, \"list_robots.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
