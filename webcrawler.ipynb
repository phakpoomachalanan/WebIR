{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "from urllib.parse import urlparse, urljoin, unquote\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "BASE_PATH = os.getenv(\"BASE_PATH\")\n",
    "BASE_HTML_PATH = os.getenv(\"BASE_HTML_PATH\")\n",
    "DOMAIN = os.getenv(\"DOMAIN\")\n",
    "MAX_PAGE = int(os.getenv(\"MAX_PAGE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Not Meow\\'s Bot',\n",
    "    'From': 'test@domain.com'\n",
    "}\n",
    "seed_url = 'http://www.ku.ac.th/th/'\n",
    "# seed_url = \"https://crawler-test.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    global headers, counter\n",
    "    text = ''\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=2)\n",
    "        # If the response was successful, no Exception will be raised\n",
    "        response.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')  # Python 3.6\n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')  # Python 3.6\n",
    "    else:\n",
    "        print(f'Success!: {(counter+1):5}, {url}')\n",
    "        text = response.text\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_base_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.scheme, parsed_url.netloc\n",
    "\n",
    "def link_parser(raw_html):\n",
    "    urls = [];\n",
    "    pattern_start = '<a href=\"';  pattern_end = '\"'\n",
    "    index = 0;  length = len(raw_html)\n",
    "    while index < length:\n",
    "        start = raw_html.find(pattern_start, index)\n",
    "        if start > 0:\n",
    "            start = start + len(pattern_start)\n",
    "            end = raw_html.find(pattern_end, start)\n",
    "            link = raw_html[start:end]\n",
    "            if len(link) > 0:\n",
    "                if link not in urls:\n",
    "                    urls.append(link)\n",
    "            index = end\n",
    "        else:\n",
    "            break\n",
    "    return urls\n",
    "\n",
    "def enqueue(links):\n",
    "    global frontier_q, visited_q\n",
    "    for link in links:\n",
    "        if link not in frontier_q and link not in visited_q:\n",
    "            frontier_q.append(link)\n",
    "\n",
    "def dequeue():\n",
    "    global frontier_q\n",
    "    current_url = frontier_q[0]\n",
    "    frontier_q = frontier_q[1:]\n",
    "    return current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(data, url):\n",
    "    global BASE_PATH\n",
    "    path = os.path.join(BASE_PATH, url)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(data)\n",
    "\n",
    "def get_dothtml_from_url(url):\n",
    "    path = urlparse(url).path\n",
    "    last_path = path.split('/')[-1]\n",
    "    if (last_path.endswith(\".html\") or last_path.endswith(\".htm\")):\n",
    "        return last_path\n",
    "    elif (\".\" in last_path):\n",
    "        return None\n",
    "    else: \n",
    "        return \"dummy\"\n",
    "    \n",
    "def decode_html_url(url):\n",
    "    url = html.unescape(url)\n",
    "    url = unquote(url)\n",
    "    return url\n",
    "\n",
    "def create_abs_url(current_url, link):\n",
    "    return urljoin(current_url, decode_html_url(link))\n",
    "\n",
    "def is_in_domain(base_url):\n",
    "    global DOMAIN\n",
    "    return base_url.endswith(\"DOMAIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier_q = [seed_url]\n",
    "visited_q = list()\n",
    "counter = 0\n",
    "has_robotstxt = list()\n",
    "has_sitemap = list()\n",
    "disallow = list()\n",
    "\n",
    "while ((len(frontier_q) != 0) and counter < MAX_PAGE):\n",
    "    current_url = dequeue()\n",
    "    if (current_url in visited_q):\n",
    "        continue\n",
    "\n",
    "    scheme, base_url = get_base_url(current_url)\n",
    "    if (not base_url.endswith(DOMAIN)):\n",
    "        continue\n",
    "\n",
    "    url = urljoin(scheme + \"://\" + base_url, \"robots.txt\")\n",
    "    if (url not in visited_q):\n",
    "        visited_q.append(url)\n",
    "        page = get_page(url)\n",
    "        if (\"User-agent:\" in page):\n",
    "            create_file(page, os.path.join(BASE_HTML_PATH, base_url, \"robots.txt\"))\n",
    "            has_robotstxt.append(base_url)\n",
    "\n",
    "        if (\"Sitemap:\" in page):\n",
    "            has_sitemap.append(base_url)\n",
    "\n",
    "    visited_q.append(current_url)\n",
    "    raw_html = get_page(current_url)\n",
    "    if (raw_html != \"\"):\n",
    "        filename = get_dothtml_from_url(current_url)\n",
    "        if (filename is None):\n",
    "            pass\n",
    "        else:\n",
    "            if (filename == \"dummy\"):\n",
    "                filename = os.path.join(\"\".join(current_url.split(\"://\")[1:]), \"dummy\")\n",
    "            else:\n",
    "                filename = \"\".join(current_url.split(\"://\")[1:])\n",
    "            counter += 1\n",
    "            create_file(raw_html, os.path.join(BASE_HTML_PATH, filename))\n",
    "\n",
    "    extracted_links = link_parser(raw_html)\n",
    "    for link in extracted_links:\n",
    "        link = create_abs_url(current_url, link)\n",
    "        ext = get_dothtml_from_url(link)\n",
    "        if (ext is None):\n",
    "            continue\n",
    "        if (link.split(\"/\")[-1].find(\"#\") != -1):\n",
    "            continue\n",
    "        enqueue([link])\n",
    "\n",
    "create_file(\"\\n\".join(has_robotstxt), os.path.join(BASE_PATH, \"list_robots.txt\"))\n",
    "create_file(\"\\n\".join(has_sitemap), os.path.join(BASE_PATH, \"list_sitemap.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
